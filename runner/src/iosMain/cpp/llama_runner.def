package = com.debanshu777.runner.cpp
---
void llama_runner_init(void);
int llama_runner_load_model(const char* model_path, int n_ctx, int n_threads, int n_batch, int n_gpu_layers, float temperature);
char* llama_runner_generate_text(const char* prompt, int max_tokens, float temperature);
void llama_runner_unload_model(void);
void llama_runner_shutdown(void);
